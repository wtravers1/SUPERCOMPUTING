I was initially drawn to Supercomputing because of the email sent out by Dr Runfola in January. Specifically, the workflow automation and data pipeline material that was mentioned seemed very useful for my data science background. I have only run programs for classes on my personal laptop, so gaining experience with a supercomputer (which I don’t know much about besides the fact it sounded really cool) seemed useful enough for me to take a class on it. I took data structures my sophomore year, and remember how our first assignment was to run a for loop, but each time we ran it the number of times the loop was iterated through increased. By the last run, I think it took my laptop around 6 hours for the loop to complete. This was my first time really thinking about time complexity, memory, and how long things actually take to run based on the hardware of the computer. I am excited to see how fast large programs can run on supercomputers that are much more capable than my 5 year old laptop.

I think the best way to learn is hands-on experience, so to keep up with the class I plan to experiment a lot in the terminal to become more comfortable with the commands and workflow. I'm coming into the class with very little experience, with my background only consisting of some beginner Python and R classes, plus some experience at an internship last summer. During this internship, I created a few Python tools that automated internal processes. To summarize briefly, I used Selenium to interact with a web browser to pull data from our internal database and store it in a SQL server and then ran algorithms to manipulate this data for different purposes. This was my first experience with Python scripts, but I quickly started to enjoy creating and using them. Something about having automation tools to make your computer do tedious jobs that I would be too lazy to do myself seems like a great idea. My boss told me that he would always choose to spend 10 hours coding a script that would save him a few minutes a day, which is a saying I couldn’t agree more with.

The folder structure I created for the first assignment follows a hierarchy very similar to the example in the assignment directions. Some specific examples from my repo are the two folders that keep raw and clean data separate, which speaking from the experience I have working with data is very useful once the data starts to pile up. This also makes it easy for others to see how your data looked at different steps and if theirs is similar as well. Also, by having script and configuration files public allows others to run the exact same code I did and verify their system is up to date. The most important part of the folder is the log file, which has an exhaustive list of every single change I made to the repository, along with notes and comments. This seems like a gold mine for someone trying to reproduce research, since they can see every single thing I did from the start of the project to the end.

I think documented code is important because it's an easy way to track progress while also making whatever you do easily replicable for others. If, for some reason, a project someone is working on blows up, they have a trail of previous changes that they can use to either revert to an earlier version or help fix their current code. This can save a research task from having to be completely restarted. Documented code is critical specifically for research because it allows other people to copy your methodology to see if they get similar results. In the first reading, the article says that research data that cannot be replicated is useless. This makes sense when you think about published research that uses results as proof of whatever they’re arguing. With no concrete documented code for others to easily copy and run themselves, it makes it very hard to verify the accuracy of the research.
